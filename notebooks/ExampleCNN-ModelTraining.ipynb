{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "p = !pwd\n",
    "p = os.path.dirname(p[0])\n",
    "if p not in sys.path:\n",
    "    sys.path.append(p)\n",
    "    \n",
    "from cnn_sys_ident.data import Dataset, MonkeyDataset\n",
    "from cnn_sys_ident.cnnsysid import ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train with all data, use Dataset.get_clean_data(). To use specific types of images for training and testing, run the commented code and secify in Dataset.add_train_test_types() the types for training and/or testing as a list. For example: Dataset.add_train_test_types(data_dict, types_train=['conv1','conv2'], types_test=['conv4']) for training with types conv1 and conv2 and testing on conv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = Dataset.load_data()\n",
    "# data_dict = Dataset.manage_repeats(data_dict)\n",
    "# data_dict = Dataset.preprocess_nans(data_dict)\n",
    "# data_dict = Dataset.add_train_test_types(data_dict, types_train='all', types_test='all')\n",
    "\n",
    "# With a wrapper function\n",
    "data_dict = Dataset.get_clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MonkeyDataset(data_dict, seed=1000, train_frac=0.8 ,subsample=2, crop = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(data, log_dir='monkey', log_hash='cnn', obs_noise_model='poisson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Log dir: %s' % model.log_hash)\n",
    "_, test_responses = data.test_av()\n",
    "_, val_responses, real_val_resps = data.val()\n",
    "_, tr_responses, real_tr_resps = data.train()\n",
    "\n",
    "val_array = data.nanarray(real_val_resps,val_responses)\n",
    "tr_array = data.nanarray(real_tr_resps,tr_responses)\n",
    "print('Average variances | validation set: %f | test set: %f' % (np.nanmean(np.nanvar(val_array, axis=0)), np.nanmean(np.nanvar(test_responses, axis=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a 3 layer CNN with filters of sizes 13x13x32, 3x3x32, and 3x3x32.\n",
    "Each convolutional layer will have stride 1, and will be padded according to 'valid', 'same', and 'same'\n",
    "Additionally, we will impose a smoothness 2d penalty of 3e-4 in the filters of the first layer and a 2.5e-4 L1 penalty on the 2nd and 3rd layer. \n",
    "The readout will have a sparsity L1 penalty of 2e-4 and finally, the output nonlinearity will have a smoothing penalty of 0 in this case (check paper for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(filter_sizes=[13, 3, 3],\n",
    "          out_channels=[32, 32, 32],\n",
    "          strides=[1, 1, 1],\n",
    "          paddings=['VALID', 'SAME', 'SAME'],\n",
    "          smooth_weights=[0.0003, 0, 0],\n",
    "          sparse_weights=[0.0, 0.00025, 0.00025],\n",
    "          readout_sparse_weight= 0.0002,\n",
    "          output_nonlin_smooth_weight = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "for lr_decay in range(3):\n",
    "    training = model.train(max_iter=10000, val_steps=100, save_steps=10000, early_stopping_steps=10, batch_size=256, learning_rate=learning_rate)\n",
    "    for (i, (logl, readout_sparse, conv_sparse, smooth, total_loss, pred)) in training:\n",
    "        result = model.eval()\n",
    "        print('Step %d | Loss: %s | %s: %s | L1 readout: %s | L1 conv: %s | L2 conv: %s | Var(test): %.4f | Var(val): %.4f' % \\\n",
    "              (i, total_loss, model.obs_noise_model, logl, readout_sparse, conv_sparse, smooth, np.mean(np.var(pred, axis=0)), np.mean(np.var(result[-1], axis=0))))\n",
    "           \n",
    "    learning_rate /= 3\n",
    "    print('Reducing learning rate to %f' % learning_rate)\n",
    "print('Done fitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Performance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.performance_test()\n",
    "eve = model.eve.mean()\n",
    "print('Explainable variance explained on test set: {}'.format(eve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.performance_val()\n",
    "eve_val = model.eve_val.mean()\n",
    "print('Explainable variance explained on validation set: {}'.format(eve_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_correlation_valset = model.evaluate_avg_corr_val()\n",
    "print('Mean single trial correlation on validation set: {}'.format(avg_correlation_valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
